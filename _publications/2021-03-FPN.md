---
title: 'Fixed Point Networks: Implicit depth models with Jacobian-free backprop.'
collection: publications
permalink: /publication/2021-03-FPN
excerpt: 'We propose a new, much faster, kind of backprop for implicit-depth neural networks.'
date: 2021-03-23
venue: '<i> (under review)</i>'
paperurl: 'https://arxiv.org/abs/2103.12803'
---

<i> Joint with Samy Wu Fung, Howard Heaton, Qiuwei Li, Stanley Osher and Wotao Yin.</i>

A growing trend in deep learning replaces fixed depth models by approximations of the limit as network depth approaches infinity. This approach uses a portion of network weights to prescribe behavior by defining a limit condition. This makes network depth implicit, varying based on the provided data and an error tolerance. We propose a new implicit depth architecture, Fixed Point Networks (FPN), and a new Jacobian-free backpropagation (JFB) scheme. JFB makes FPNs much faster and easier to train.

[Arxiv version](https://arxiv.org/abs/2103.12803)
